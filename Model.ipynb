{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook we will see two Machine Learning models to predict manuscript digits, training with **MNIST** Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MNIST** data set is available on package from _**scikit-learn**_ distribution, so we can import data directly. We have to normalize data between 0 and 1, so we have to divide by 255 each pixel. We also prepare the arrays for the first model, newral networks densely connected, so we need to have all the data in just a single array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test)= mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test  = X_test.reshape (10000, 784)\n",
    "\n",
    "X_train = X_train.astype ('float32' )\n",
    "X_test  = X_test.astype  ('float32' )\n",
    "\n",
    "X_train/= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have loaded the data, we can go ahead and define and train the model. Specifically we have to connect the input layer (dimensions $28\\cdot 28=784$ pixels) with two hidden layers, and with the output layer, where we apply _**softmax**_.\n",
    "\n",
    "Additionally, in order to validate correctly, we have to convert labels to dummies.\n",
    "\n",
    "Now we can compile the net specifying **loss**, **optimizer** and **metrics**.\n",
    "\n",
    "And then we are ready to train the model. We can use the Keras _**evaluate**_ method in order to validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 109,386\n",
      "Trainable params: 109,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: 0.2422 - accuracy: 0.9295 - val_loss: 0.1261 - val_accuracy: 0.9619\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.1015 - accuracy: 0.9691 - val_loss: 0.0908 - val_accuracy: 0.9717\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.0711 - accuracy: 0.9775 - val_loss: 0.0767 - val_accuracy: 0.9760\n",
      "Test loss: 0.07671645826920867\n",
      "Test accuracy: 0.9760000109672546\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "num_classes=10\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train_cat=keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_cat =keras.utils.to_categorical(y_test , num_classes)\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(784,)))\n",
    "model.add(Dense(64 , activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_cat, epochs=3, verbose=1, validation_data=(X_test, y_test_cat))\n",
    "\n",
    "score=model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "print('Test loss:'    , score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have obtained good results, but we will calculate the _**confusion matrix**_ to see the error distribution in the different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       980\n",
      "           1       0.99      0.98      0.99      1135\n",
      "           2       0.96      0.98      0.97      1032\n",
      "           3       0.97      0.97      0.97      1010\n",
      "           4       0.98      0.98      0.98       982\n",
      "           5       0.98      0.96      0.97       892\n",
      "           6       0.98      0.98      0.98       958\n",
      "           7       0.98      0.97      0.97      1028\n",
      "           8       0.95      0.98      0.96       974\n",
      "           9       0.98      0.96      0.97      1009\n",
      "\n",
      "    accuracy                           0.98     10000\n",
      "   macro avg       0.98      0.98      0.98     10000\n",
      "weighted avg       0.98      0.98      0.98     10000\n",
      "\n",
      "     0     1     2    3    4    5    6    7    8    9\n",
      "0  973     1     0    0    0    0    2    1    2    1\n",
      "1    0  1115     5    0    0    1    2    1   11    0\n",
      "2    4     0  1016    1    3    0    1    5    2    0\n",
      "3    0     0    11  979    0    4    0    4   10    2\n",
      "4    3     0     5    0  961    0    3    5    1    4\n",
      "5    4     0     0   15    1  857    6    2    5    2\n",
      "6    7     2     1    1    4    2  939    0    2    0\n",
      "7    1     3    10    5    0    0    0  999    4    6\n",
      "8    2     0     7    1    2    5    2    2  951    2\n",
      "9    4     2     0    4   10    5    0    4   10  970\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred=model.predict_classes(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "conf=pd.DataFrame(\n",
    "    confusion_matrix(y_test, y_pred),\n",
    ")\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will try with convolutional layers. In this case, because of the input layer is a convolutional layer, we have to prepare the data to be bidimensional arrays (matrix), instead of unidimendionals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test =X_test.reshape (X_test.shape [0], 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is to make sure that we don't have any compatibility problems among our actual Keras version and the last released version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.__version__ is 2.0.0\n",
      "tf.keras.__version__ is: 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as tfback\n",
    "\n",
    "print(\"tf.__version__ is\", tf.__version__)\n",
    "print(\"tf.keras.__version__ is:\", tf.keras.__version__)\n",
    "\n",
    "def _get_available_gpus():\n",
    "    \"\"\"Get a list of available gpu devices (formatted as strings).\n",
    "\n",
    "    # Returns\n",
    "        A list of available GPU devices.\n",
    "    \"\"\"\n",
    "    return []\n",
    "    #global _LOCAL_DEVICES\n",
    "    if tfback._LOCAL_DEVICES is None:\n",
    "        devices = tf.config.list_logical_devices()\n",
    "        tfback._LOCAL_DEVICES = [x.name for x in devices]\n",
    "    return [x for x in tfback._LOCAL_DEVICES if 'device:gpu' in x.lower()]\n",
    "\n",
    "tfback._get_available_gpus = _get_available_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 20, 24, 24)        520       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 10, 12, 24)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 6, 8, 50)          30050     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 4, 50)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 500)               300500    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                5010      \n",
      "=================================================================\n",
      "Total params: 336,080\n",
      "Trainable params: 336,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 165s 3ms/step - loss: 0.1596 - accuracy: 0.9505 - val_loss: 0.0614 - val_accuracy: 0.9800\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 160s 3ms/step - loss: 0.0563 - accuracy: 0.9823 - val_loss: 0.0415 - val_accuracy: 0.9869\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 160s 3ms/step - loss: 0.0384 - accuracy: 0.9881 - val_loss: 0.0353 - val_accuracy: 0.9893\n",
      "Test loss: 0.03531677504512918\n",
      "Test accuracy: 0.989300012588501\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "num_classes=10\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train_cat=keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_cat =keras.utils.to_categorical(y_test , num_classes)\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Conv2D(20, (5, 5), activation='relu', input_shape=(1, 28, 28), data_format='channels_first'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Conv2D(50, (5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_cat, epochs=3, verbose=1, validation_data=(X_test, y_test_cat))\n",
    "\n",
    "score=model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "print('Test loss:'    , score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this model, we have better results but it's also slower and computationally more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       980\n",
      "           1       0.99      1.00      1.00      1135\n",
      "           2       0.99      0.99      0.99      1032\n",
      "           3       0.99      0.99      0.99      1010\n",
      "           4       0.99      1.00      0.99       982\n",
      "           5       0.99      0.99      0.99       892\n",
      "           6       0.99      0.99      0.99       958\n",
      "           7       0.99      0.99      0.99      1028\n",
      "           8       0.98      0.99      0.99       974\n",
      "           9       0.99      0.98      0.98      1009\n",
      "\n",
      "    accuracy                           0.99     10000\n",
      "   macro avg       0.99      0.99      0.99     10000\n",
      "weighted avg       0.99      0.99      0.99     10000\n",
      "\n",
      "     0     1     2    3    4    5    6     7    8    9\n",
      "0  968     1     0    0    0    1    5     2    2    1\n",
      "1    0  1132     2    0    0    1    0     0    0    0\n",
      "2    1     1  1023    0    0    0    0     4    2    1\n",
      "3    0     0     2  997    0    4    0     1    4    2\n",
      "4    0     0     1    0  978    0    3     0    0    0\n",
      "5    0     0     0    4    0  881    1     1    3    2\n",
      "6    1     2     1    0    2    1  949     0    2    0\n",
      "7    0     4     7    0    0    0    0  1013    1    3\n",
      "8    1     0     2    2    1    0    0     3  962    3\n",
      "9    0     0     0    3    9    1    0     4    2  990\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict_classes(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "conf=pd.DataFrame(\n",
    "    confusion_matrix(y_test, y_pred),\n",
    ")\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will save the model in **.pkl** format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(model, open('convolutional_nn_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can load the model again like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = pickle.load(open('convolutional_nn_model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can see this model with an UI here: <a href=\"https://mnistplayground.herokuapp.com/\" target=\"_blank\">MNIST Playground</a>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
